{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48525036",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'env (Python 3.13.5)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sudip/OneDrive/Desktop/palm-recognition-csy2082/env/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow/Keras imports for deep learning\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Scikit-learn imports for data splitting and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f57b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom preprocessing module for palm images\n",
    "DATASET_PATH = \"dataset\"\n",
    "from img_preprocessing import PalmPreprocessor\n",
    "\n",
    "# ========== DATA LOADING AND PREPROCESSING ==========\n",
    "# Initialize lists to store images and corresponding labels\n",
    "images, labels = [], []\n",
    "# Get sorted list of class names (person names) from dataset directory\n",
    "class_names = sorted(os.listdir(DATASET_PATH))\n",
    "\n",
    "# Iterate through each class (person) in the dataset\n",
    "for idx, cls in enumerate(class_names):\n",
    "    cls_path = os.path.join(DATASET_PATH, cls)  # Path to current class folder\n",
    "\n",
    "    # Process each image in the current class folder\n",
    "    for img_name in os.listdir(cls_path):\n",
    "        img_path = os.path.join(cls_path, img_name)  # Full path to image\n",
    "        \n",
    "        # Use custom preprocessor to preprocess the palm image\n",
    "        processed = PalmPreprocessor().preprocess_v2(img_path)\n",
    "\n",
    "        # Skip if preprocessing fails\n",
    "        if processed is None:\n",
    "            import time\n",
    "            time.sleep(5)  # Wait before trying next image\n",
    "            continue\n",
    "\n",
    "        # Add processed image and label to lists\n",
    "        images.append(processed)\n",
    "        labels.append(idx)  # Use index as numerical label\n",
    "\n",
    "# Convert lists to numpy arrays for model training\n",
    "images = np.array(images)\n",
    "# Convert labels to one-hot encoded format (e.g., [0,0,1,0] for class 2)\n",
    "labels = to_categorical(labels, num_classes=len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4f240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DATA SPLITTING ==========\n",
    "# Split data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images, labels, test_size=0.2, random_state=42, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8529a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size (model expects 128x128 grayscale images)\n",
    "IMG_SIZE = 128\n",
    "\n",
    "\n",
    "# ========== CNN MODEL ARCHITECTURE ==========\n",
    "# Create Sequential model (linear stack of layers)\n",
    "model = Sequential([\n",
    "    # First convolutional layer: 32 filters, 3x3 kernel, ReLU activation\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "    # Max pooling to reduce spatial dimensions by half\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    # Second convolutional layer: 64 filters\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    # Third convolutional layer: 128 filters\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    # Flatten 3D feature maps to 1D vector for dense layers\n",
    "    Flatten(),\n",
    "    # Fully connected layer with 128 neurons\n",
    "    Dense(128, activation='relu'),\n",
    "    # Dropout layer to prevent overfitting (50% neurons randomly disabled)\n",
    "    Dropout(0.5),\n",
    "    # Output layer with softmax activation for multi-class classification\n",
    "    Dense(len(class_names), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Low learning rate for stable training\n",
    "    loss='categorical_crossentropy',       # Standard loss for multi-class classification\n",
    "    metrics=['accuracy']                   # Track accuracy during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add1138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model architecture summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f342365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MODEL TRAINING ==========\n",
    "# Train the model for 20 epochs with 16 samples per batch\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2  # Use 20% of training data for validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TRAINING VISUALIZATION ==========\n",
    "# Create figure with 2 subplots for training metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Training and Validation Accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_title(\"Model Accuracy\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)  # Add subtle grid for better readability\n",
    "\n",
    "# Plot 2: Training and Validation Loss\n",
    "axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].set_title(\"Model Loss\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a94ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MODEL EVALUATION ==========\n",
    "# Evaluate model performance on test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Generate predictions for test set\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)  # Get class index with highest probability\n",
    "y_true = np.argmax(y_test, axis=1)  # Convert one-hot back to class indices\n",
    "\n",
    "# Display confusion matrix and classification report\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SAVE TRAINED MODEL ==========\n",
    "# Save the complete trained model to disk for later use\n",
    "model.save(\"palm_recognition_model.h5\")\n",
    "print(\"✓ Model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fe22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CREATE EMBEDDING MODEL ==========\n",
    "# Load the saved model fresh to ensure proper state\n",
    "model_for_prediction = load_model(\"palm_recognition_model.h5\")\n",
    "\n",
    "# Create a new model that outputs embeddings (features from the second-to-last layer)\n",
    "# This is useful for similarity comparisons and face/palm recognition systems\n",
    "embedding_model = Model(\n",
    "    inputs=model_for_prediction.inputs,           # Same input as original model\n",
    "    outputs=model_for_prediction.layers[-2].output  # Output from Dense(128) layer\n",
    ")\n",
    "\n",
    "# ========== GENERATE REFERENCE EMBEDDINGS ==========\n",
    "# Create a dictionary to store average embeddings for each person\n",
    "reference_embeddings = {}\n",
    "EMBEDDING_THRESHOLD = 1.2  # Threshold for similarity matching (not used here but defined for future)\n",
    "\n",
    "# Generate embeddings for each person in the dataset\n",
    "for person in class_names:\n",
    "    embeddings = []\n",
    "    person_path = os.path.join(DATASET_PATH, person)\n",
    "    \n",
    "    # Process all images for the current person\n",
    "    for img_name in os.listdir(person_path):\n",
    "        # Skip non-image files\n",
    "        if not img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            continue\n",
    "            \n",
    "        img_path = os.path.join(person_path, img_name)\n",
    "        \n",
    "        # Use same preprocessing as during training\n",
    "        preprocessor = PalmPreprocessor()\n",
    "        processed = preprocessor.preprocess_v2(img_path)\n",
    "        \n",
    "        if processed is None:\n",
    "            continue\n",
    "            \n",
    "        # Reshape to match model input shape: (1, 128, 128, 1)\n",
    "        if len(processed.shape) == 2:\n",
    "            processed = processed.reshape(1, IMG_SIZE, IMG_SIZE, 1)\n",
    "        else:\n",
    "            processed = processed.reshape(1, IMG_SIZE, IMG_SIZE, 1)\n",
    "        \n",
    "        # Extract embedding (128-dimensional feature vector)\n",
    "        emb = embedding_model.predict(processed, verbose=0)[0]\n",
    "        embeddings.append(emb)\n",
    "    \n",
    "    # Store average embedding if valid images were found\n",
    "    if embeddings:\n",
    "        reference_embeddings[person] = np.mean(embeddings, axis=0)\n",
    "        print(f\"✓ Generated embeddings for {person}: {len(embeddings)} samples\")\n",
    "    else:\n",
    "        print(f\"✗ No valid images found for {person}\")\n",
    "\n",
    "# Save embeddings to disk for later use in recognition\n",
    "with open(\"reference_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reference_embeddings, f)\n",
    "print(\"✓ Embeddings saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c54fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TEST PREDICTION WITH RANDOM SAMPLE ==========\n",
    "# Threshold for classifying as \"Unknown\" (if confidence is too low)\n",
    "CONFIDENCE_THRESHOLD = 0.6\n",
    "\n",
    "# Randomly select a class for testing\n",
    "true_label = random.choice(class_names)\n",
    "class_path = os.path.join(DATASET_PATH, true_label)\n",
    "\n",
    "# Get list of valid image files in the selected class\n",
    "valid_images = [f for f in os.listdir(class_path) \n",
    "                if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "if not valid_images:\n",
    "    print(f\"No valid images found in {class_path}\")\n",
    "else:\n",
    "    # Randomly select an image from the class\n",
    "    img_name = random.choice(valid_images)\n",
    "    img_path = os.path.join(class_path, img_name)\n",
    "    \n",
    "    # Preprocess the test image using the same pipeline\n",
    "    preprocessor = PalmPreprocessor()\n",
    "    processed = preprocessor.preprocess_v2(img_path)\n",
    "    \n",
    "    if processed is not None:\n",
    "        # Reshape for model input\n",
    "        if len(processed.shape) == 2:\n",
    "            img_input = processed.reshape(1, IMG_SIZE, IMG_SIZE, 1)\n",
    "        else:\n",
    "            img_input = processed.reshape(1, IMG_SIZE, IMG_SIZE, 1)\n",
    "        \n",
    "        # Get prediction probabilities for all classes\n",
    "        predictions = model_for_prediction.predict(img_input, verbose=0)[0]\n",
    "        confidence = np.max(predictions)  # Highest probability\n",
    "        predicted_index = np.argmax(predictions)  # Index of predicted class\n",
    "        \n",
    "        # Apply unknown detection logic\n",
    "        if confidence < CONFIDENCE_THRESHOLD:\n",
    "            predicted_label = \"Unknown\"  # Classify as unknown if confidence is low\n",
    "        else:\n",
    "            predicted_label = class_names[predicted_index]  # Otherwise use predicted class\n",
    "        \n",
    "        # Load original image for visualization\n",
    "        img = cv2.imread(img_path)\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Create visualization with original and processed images\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        \n",
    "        # Plot original image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_gray, cmap='gray')\n",
    "        plt.title(f\"Original\\nClass: {true_label}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Plot processed image with prediction results\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(processed.squeeze(), cmap='gray')\n",
    "        plt.title(f\"Processed\\nPred: {predicted_label}\\nConf: {confidence:.2f}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed prediction results\n",
    "        print(f\"\\nPrediction Results:\")\n",
    "        print(f\"  Original class: {true_label}\")\n",
    "        print(f\"  Predicted: {predicted_label}\")\n",
    "        print(f\"  Confidence: {confidence:.2%}\")\n",
    "        print(f\"  Correct: {true_label == predicted_label}\")\n",
    "        \n",
    "        # Display probabilities for all classes\n",
    "        print(\"\\nAll class probabilities:\")\n",
    "        for i, cls in enumerate(class_names):\n",
    "            print(f\"  {cls}: {predictions[i]:.2%}\")\n",
    "    else:\n",
    "        print(f\"Failed to preprocess image: {img_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
